{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will learn who to generate our submission from computed features of the dataset.\n",
    "\n",
    "First, import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import List, Tuple, Dict, Union\n",
    "\n",
    "# from src.utils import fit_lr_classifier, infer_lr_classifier, calculate_score, calculate_score_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define somm helper functions. They can be found at `src.utils.fitter` and `src.utils.io`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_score_raw(Y_dev: np.ndarray, \n",
    "                        Y_pred: np.ndarray):\n",
    "    \"\"\"Calculate loss, accuracy and f1_score of prediction\n",
    "\n",
    "    Args:\n",
    "        Y_dev (np.ndarray): label\n",
    "        Y_pred (np.ndarray): prediction\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int, int]: loss, accuracy and f1_score\n",
    "    \"\"\"\n",
    "    loss_val = metrics.log_loss(Y_dev, Y_pred)\n",
    "    Y_pred = (Y_pred > 0.5).astype(int)\n",
    "    f1_score = metrics.f1_score(Y_dev, Y_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "    acc = metrics.accuracy_score(Y_dev, Y_pred)\n",
    "    print(f\"Loss: {loss_val}, Accuracy: {acc}, F1-score: {f1_score}\")\n",
    "    return loss_val, acc, f1_score\n",
    "\n",
    "\n",
    "def calculate_score(clf: LogisticRegression,\n",
    "                    X_dev: np.ndarray, \n",
    "                    Y_dev: np.ndarray):\n",
    "    \"\"\"Test a classifier on dev set\n",
    "\n",
    "    Args:\n",
    "        clf (LogisticRegression): classifier\n",
    "        X_dev (np.ndarray): input\n",
    "        Y_dev (np.ndarray): label\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int, int]: loss, accuracy and f1_score\n",
    "    \"\"\"\n",
    "    Y_pred = clf.predict_proba(X_dev)\n",
    "    Y_pred = Y_pred[:, 1]\n",
    "\n",
    "    loss_val, acc, f1_score = calculate_score_raw(Y_dev, Y_pred)\n",
    "    return loss_val, acc, f1_score\n",
    "\n",
    "\n",
    "def fit_lr_classifier(X_train: np.ndarray,\n",
    "                      Y_train: np.ndarray,\n",
    "                      X_dev: np.ndarray = None,\n",
    "                      Y_dev: np.ndarray = None,\n",
    "                      *args,\n",
    "                      **kwargs):\n",
    "    \"\"\"Fit a classifier using X_train and Y_train, teset on X_dev (if available). The hyperparameters is selected from args and kwargs\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): train input\n",
    "        Y_train (np.ndarray): train label\n",
    "        X_dev (np.ndarray, optional): dev input. Defaults to None.\n",
    "        Y_dev (np.ndarray, optional): dev label. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        LogisticRegression: trained classifier\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(*args, **kwargs)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    if X_dev is not None and Y_dev is not None:\n",
    "        calculate_score(clf, X_dev, Y_dev)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def infer_lr_classifier(clf, X_test):\n",
    "    \"\"\"User a classifier to infer\n",
    "\n",
    "    Args:\n",
    "        clf (LogisticRegression): classifier\n",
    "        X_test (np.ndarray): input\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: prediction [Nx1]\n",
    "    \"\"\"\n",
    "    pred = clf.predict_proba(X_test)\n",
    "    return pred[:, 1]\n",
    "\n",
    "def generate_submission(output_dir: str, \n",
    "                        pred: np.ndarray, \n",
    "                        tag: str = \"\"):\n",
    "    \"\"\"The function dumps a prediction tensor to submission csv\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): directory of ouput\n",
    "        pred (np.ndarray): prediction\n",
    "        tag (str, optional): tag to append before filename. Defaults to \"\".\n",
    "    \"\"\"\n",
    "    if not (len(pred.shape) == 1 or (len(pred.shape) == 2 and pred.shape[1] == 1)):\n",
    "        print(f\"Expect pred to be a vector\")\n",
    "    if not pred.shape[0] == 106692:\n",
    "        print(f\"Expect pred to have length 106692 but have {pred.shape[0]}\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    results = zip(range(len(pred)), pred)\n",
    "    with open(os.path.join(output_dir, f'{\"_\".join([tag, \"submission\"])}.csv' if len(tag) > 0 else 'submission.csv'), 'w') as f:\n",
    "        f.write('id,predicted\\n')\n",
    "        for idx, row in enumerate(results):\n",
    "            f.write(f\"{row[0]},{row[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You have to provide the features and put them under the `features` directory. Refer the `README.md` for instruactions\n",
    "\n",
    "| Name                                    | Content                                           |\n",
    "| --------------------------------------- | ------------------------------------------------- |\n",
    "| `features/baseline-enhanced.pkl`        | from `{PROJECT_ROOT}/tasks/run_new_baseline`      |\n",
    "| `features/author_graph_lr_features.pkl` | from `{PROJECT_ROOT}/tasks/run_authorgraph_lr`    |\n",
    "| `features/graphsage_essay_features.pkl` | from `{PROJECT_ROOT}/tasks/train_graphsage_essay` |\n",
    "| `features/uv_list.pkl`                  | from `{PROJECT_ROOT}/tasks/generate_dataset`      |\n",
    "\n",
    "> `author_graph_lr_features.pkl` and `author_graph_lr_features.pkl` are selective\n",
    "\n",
    "There are two way to get these features:\n",
    "\n",
    "**1. Download**\n",
    "\n",
    "Download the pre-computed features from [https://github.com/davidliyutong/ICE6407P-260-M01/releases/tag/submission](https://github.com/davidliyutong/ICE6407P-260-M01/releases/tag/submission)\n",
    "\n",
    "Unzip the `features.zip`, then put all `*.pkl` files under the `./features` directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!bash ./startup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**2. Generation**\n",
    "\n",
    "By running a set of pythonscripts, you can generate these features. See `manual_generation.md` for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Enhanced baseline method\n",
    "\n",
    "The quickest run the enhanced baseline method is the run the `{PROJECT_ROOT}/tasks/run_new_baseline/run_new_baseline.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('../../')\n",
    "%run ../run_new_baseline/run_new_baseline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load dataset from serialized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features = {\n",
    "    'author_graph_lr': pickle.load(open('./features/author_graph_lr_features.pkl', 'rb')),\n",
    "    'graphsage_essay': pickle.load(open('./features/graphsage_essay_features.pkl', 'rb')),\n",
    "    'baseline_enhanced': pickle.load(open('./features/baseline-enhanced.pkl','rb'))\n",
    "}\n",
    "uv_list = pickle.load(open('./features/uv_list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_0 = features['baseline_enhanced']['X_train']\n",
    "X_dev_0 = features['baseline_enhanced']['X_dev']\n",
    "X_test_0 =  features['baseline_enhanced']['X_test']\n",
    "X_whole_0 = np.concatenate([features['baseline_enhanced']['X_train'],\n",
    "                            features['baseline_enhanced']['X_dev']],axis=0)\n",
    "\n",
    "X_train_1 = features['graphsage_essay']['X_train']\n",
    "X_dev_1 = features['graphsage_essay']['X_dev']\n",
    "X_test_1 =  features['graphsage_essay']['X_test']\n",
    "X_whole_1 = np.concatenate([features['graphsage_essay']['X_train'],\n",
    "                            features['graphsage_essay']['X_dev']],axis=0)\n",
    "\n",
    "\n",
    "X_train_2 = features['author_graph_lr']['X_train']\n",
    "X_dev_2 = features['author_graph_lr']['X_dev']\n",
    "X_test_2 = features['author_graph_lr']['X_test']\n",
    "X_whole_2 = np.concatenate([features['author_graph_lr']['X_train'],\n",
    "                            features['author_graph_lr']['X_dev']],axis=0)\n",
    "X_train_3 = np.concatenate([X_train_0, X_train_2], axis=1)\n",
    "X_dev_3 = np.concatenate([X_dev_0, X_dev_2], axis=1)\n",
    "X_test_3 =  np.concatenate([X_test_0, X_test_2], axis=1)\n",
    "X_whole_3 =  np.concatenate([X_whole_0, X_whole_2], axis=1)\n",
    "\n",
    "Y_train = uv_list['train_y']\n",
    "Y_dev = uv_list['dev_y']\n",
    "Y_whole = np.concatenate([Y_train, Y_dev], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train the LR classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf0 = fit_lr_classifier(\n",
    "    X_whole_0,\n",
    "    Y_whole,\n",
    "    X_dev_0,\n",
    "    Y_dev,\n",
    "    solver='lbfgs',\n",
    "    tol=1e-5,\n",
    "    max_iter=1000,\n",
    "    n_jobs=8,\n",
    "    verbose=1,\n",
    ")\n",
    "clf1 = fit_lr_classifier(\n",
    "    X_whole_1,\n",
    "    Y_whole,\n",
    "    X_dev_1,\n",
    "    Y_dev,\n",
    "    tol=1e-5,\n",
    "    max_iter=600,\n",
    "    verbose=1,\n",
    ")\n",
    "clf2 = fit_lr_classifier(\n",
    "    X_whole_2, \n",
    "    Y_whole, \n",
    "    X_dev_2, \n",
    "    Y_dev, \n",
    "    tol=1e-5,\n",
    "    max_iter=600, \n",
    "    verbose=1,\n",
    ")\n",
    "clf3 = fit_lr_classifier(\n",
    "    X_whole_3,\n",
    "    Y_whole,\n",
    "    X_dev_3,\n",
    "    Y_dev,\n",
    "    solver='lbfgs',\n",
    "    tol=1e-5,\n",
    "    max_iter=400,\n",
    "    n_jobs=8,\n",
    "    verbose=1,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = infer_lr_classifier(clf3, X_test_3)\n",
    "generate_submission('./outputs', scores, \"all_gather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = infer_lr_classifier(clf0, X_test_0)\n",
    "generate_submission('./outputs', scores, \"baseline_enhanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n",
    "\n",
    "We could use the voting techniques to aggregate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Voter:\n",
    "    def __init__(self, estimators: list, weight=None):\n",
    "        self.estimators_lookup = {name: idx for idx, (name, _) in enumerate(estimators)}\n",
    "        self.estimators = [item for name, item in estimators]\n",
    "        if weight is not None:\n",
    "            self.weight = weight / sum(weight)\n",
    "        else:\n",
    "            self.weight = None\n",
    "\n",
    "    def fit(self, x, y, *args, **kwargs):\n",
    "        for name, data in x.keys():\n",
    "            self.estimators[self.estimators_lookup[name]].fit(data, y, *args, **kwargs)\n",
    "            print(f\"fitting: {name}\")\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        predictions = [self.estimators[self.estimators_lookup[name]].predict_proba(x[name]) for name in x.keys()]\n",
    "        summary = 0\n",
    "\n",
    "        for idx, partial_result in enumerate(predictions):\n",
    "            if self.weight is not None:\n",
    "                summary += partial_result * self.weight[idx]\n",
    "            else:\n",
    "                summary += 1 / len(self.estimators) * partial_result\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def decide(self, x, thresh=0.5):\n",
    "        predictions = [self.estimators[self.estimators_lookup[name]].predict_proba(x[name]) for name in x.keys()]\n",
    "        summary = 0\n",
    "\n",
    "        for idx, partial_result in enumerate(predictions):\n",
    "            if self.weight is not None:\n",
    "                summary += partial_result * self.weight[idx]\n",
    "            else:\n",
    "                summary += 1 / len(self.estimators) * partial_result\n",
    "\n",
    "        return summary[:, 1] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "voter = Voter(estimators=[('lr0', clf0), ('lr1', clf1), ('lr2', clf2)],weight=np.array([100,87,88]))\n",
    "pred = voter.decide({'lr0': X_dev_0, 'lr1': X_dev_1, 'lr2': X_dev_2})\n",
    "calculate_score_raw(pred, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = voter.predict_proba({'lr0': X_test_0, 'lr1': X_test_1, 'lr2': X_test_2})[:,1]\n",
    "generate_submission('./outputs', scores, \"voting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}